{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python [conda env:pytorch4]",
      "language": "python",
      "name": "conda-env-pytorch4-py"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.5.5"
    },
    "colab": {
      "name": "3.ppo.ipynb",
      "provenance": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "PbaHh0K1LqEY",
        "outputId": "554bb1fe-dc70-4060-fbe5-d8fc42b30715",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!pip install git+https://github.com/StevenJokess/RL-Adventure-2\r\n",
        "!git clone https://github.com/StevenJokess/RL-Adventure-2\r\n",
        "%cd RL-Adventure-2\r\n",
        "!ls"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting git+https://github.com/StevenJokess/RL-Adventure-2\n",
            "  Cloning https://github.com/StevenJokess/RL-Adventure-2 to /tmp/pip-req-build-56voypy5\n",
            "  Running command git clone -q https://github.com/StevenJokess/RL-Adventure-2 /tmp/pip-req-build-56voypy5\n",
            "\u001b[31mERROR: Command errored out with exit status 1: python setup.py egg_info Check the logs for full command output.\u001b[0m\n",
            "fatal: destination path 'RL-Adventure-2' already exists and is not an empty directory.\n",
            "/content/RL-Adventure-2\n",
            "'1.actor-critic(colab).ipynb'   4.acer.ipynb\t\t     9.her.ipynb\n",
            " 1.actor-critic.ipynb\t        5.ddpg.ipynb\t\t     common\n",
            "'2.gae(colab).ipynb'\t        6.td3.ipynb\t\t     README.md\n",
            " 2.gae.ipynb\t\t       '7.soft actor-critic.ipynb'\n",
            " 3.ppo.ipynb\t\t        8.gail.ipynb\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q07DBS6-LUWr"
      },
      "source": [
        "import math\n",
        "import random\n",
        "\n",
        "import gym\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.distributions import Normal"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_l_OoOP7LUW0"
      },
      "source": [
        "from IPython.display import clear_output\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4P1Pfq0ELUW1"
      },
      "source": [
        "<h2>Use CUDA</h2>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Mr4HFkRLUW2"
      },
      "source": [
        "use_cuda = torch.cuda.is_available()\n",
        "device   = torch.device(\"cuda\" if use_cuda else \"cpu\")"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Zryqg1ULUW2"
      },
      "source": [
        "<h2>Create Environments</h2>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WGNIWlbiLUW2"
      },
      "source": [
        "from common.multiprocessing_env import SubprocVecEnv\n",
        "\n",
        "num_envs = 16\n",
        "env_name = \"Pendulum-v0\"\n",
        "\n",
        "def make_env():\n",
        "    def _thunk():\n",
        "        env = gym.make(env_name)\n",
        "        return env\n",
        "\n",
        "    return _thunk\n",
        "\n",
        "envs = [make_env() for i in range(num_envs)]\n",
        "envs = SubprocVecEnv(envs)\n",
        "\n",
        "env = gym.make(env_name)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Vt4ODueLUW3"
      },
      "source": [
        "<h2>Neural Network</h2>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G1j_Z3O3LUW3"
      },
      "source": [
        "def init_weights(m):\n",
        "    if isinstance(m, nn.Linear):\n",
        "        nn.init.normal_(m.weight, mean=0., std=0.1)\n",
        "        nn.init.constant_(m.bias, 0.1)\n",
        "        \n",
        "\n",
        "class ActorCritic(nn.Module):\n",
        "    def __init__(self, num_inputs, num_outputs, hidden_size, std=0.0):\n",
        "        super(ActorCritic, self).__init__()\n",
        "        \n",
        "        self.critic = nn.Sequential(\n",
        "            nn.Linear(num_inputs, hidden_size),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_size, 1)\n",
        "        )\n",
        "        \n",
        "        self.actor = nn.Sequential(\n",
        "            nn.Linear(num_inputs, hidden_size),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_size, num_outputs),\n",
        "        )\n",
        "        self.log_std = nn.Parameter(torch.ones(1, num_outputs) * std)\n",
        "        \n",
        "        self.apply(init_weights)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        value = self.critic(x)\n",
        "        mu    = self.actor(x)\n",
        "        std   = self.log_std.exp().expand_as(mu)\n",
        "        dist  = Normal(mu, std)\n",
        "        return dist, value"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mZXs_HY6LUW3"
      },
      "source": [
        "def plot(frame_idx, rewards):\n",
        "    clear_output(True)\n",
        "    plt.figure(figsize=(20,5))\n",
        "    plt.subplot(131)\n",
        "    plt.title('frame %s. reward: %s' % (frame_idx, rewards[-1]))\n",
        "    plt.plot(rewards)\n",
        "    plt.show()\n",
        "    \n",
        "def test_env(vis=False):\n",
        "    state = env.reset()\n",
        "    if vis: env.render()\n",
        "    done = False\n",
        "    total_reward = 0\n",
        "    while not done:\n",
        "        state = torch.FloatTensor(state).unsqueeze(0).to(device)\n",
        "        dist, _ = model(state)\n",
        "        next_state, reward, done, _ = env.step(dist.sample().cpu().numpy()[0])\n",
        "        state = next_state\n",
        "        if vis: env.render()\n",
        "        total_reward += reward\n",
        "    return total_reward"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zEwDh2ZGLUW4"
      },
      "source": [
        "<h2>GAE</h2>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yDEGeHD6LUW4"
      },
      "source": [
        "def compute_gae(next_value, rewards, masks, values, gamma=0.99, tau=0.95):\n",
        "    values = values + [next_value]\n",
        "    gae = 0\n",
        "    returns = []\n",
        "    for step in reversed(range(len(rewards))):\n",
        "        delta = rewards[step] + gamma * values[step + 1] * masks[step] - values[step]\n",
        "        gae = delta + gamma * tau * masks[step] * gae\n",
        "        returns.insert(0, gae + values[step])\n",
        "    return returns"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZFOVwofxLUW5"
      },
      "source": [
        "<h1> Proximal Policy Optimization Algorithm</h1>\n",
        "<h2><a href=\"https://arxiv.org/abs/1707.06347\">Arxiv</a></h2>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rO9VDU-kLUW6"
      },
      "source": [
        "def ppo_iter(mini_batch_size, states, actions, log_probs, returns, advantage):\n",
        "    batch_size = states.size(0)\n",
        "    for _ in range(batch_size // mini_batch_size):\n",
        "        rand_ids = np.random.randint(0, batch_size, mini_batch_size)\n",
        "        yield states[rand_ids, :], actions[rand_ids, :], log_probs[rand_ids, :], returns[rand_ids, :], advantage[rand_ids, :]\n",
        "        \n",
        "        \n",
        "\n",
        "def ppo_update(ppo_epochs, mini_batch_size, states, actions, log_probs, returns, advantages, clip_param=0.2):\n",
        "    for _ in range(ppo_epochs):\n",
        "        for state, action, old_log_probs, return_, advantage in ppo_iter(mini_batch_size, states, actions, log_probs, returns, advantages):\n",
        "            dist, value = model(state)\n",
        "            entropy = dist.entropy().mean()\n",
        "            new_log_probs = dist.log_prob(action)\n",
        "\n",
        "            ratio = (new_log_probs - old_log_probs).exp()\n",
        "            surr1 = ratio * advantage\n",
        "            surr2 = torch.clamp(ratio, 1.0 - clip_param, 1.0 + clip_param) * advantage\n",
        "\n",
        "            actor_loss  = - torch.min(surr1, surr2).mean()\n",
        "            critic_loss = (return_ - value).pow(2).mean()\n",
        "\n",
        "            loss = 0.5 * critic_loss + actor_loss - 0.001 * entropy\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9XNfADArLUW7"
      },
      "source": [
        "num_inputs  = envs.observation_space.shape[0]\n",
        "num_outputs = envs.action_space.shape[0]\n",
        "\n",
        "#Hyper params:\n",
        "hidden_size      = 256\n",
        "lr               = 3e-4\n",
        "num_steps        = 20\n",
        "mini_batch_size  = 5\n",
        "ppo_epochs       = 4\n",
        "threshold_reward = -200\n",
        "\n",
        "model = ActorCritic(num_inputs, num_outputs, hidden_size).to(device)\n",
        "optimizer = optim.Adam(model.parameters(), lr=lr)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wE7Ly8vhLUW7"
      },
      "source": [
        "max_frames = 4000 #15000\n",
        "frame_idx  = 0\n",
        "test_rewards = []"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FN55ZWVJLUW8",
        "outputId": "e22faf95-bc0a-4749-dcf4-8b9c57f99b3a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 336
        }
      },
      "source": [
        "state = envs.reset()\n",
        "early_stop = False\n",
        "\n",
        "while frame_idx < max_frames and not early_stop:\n",
        "\n",
        "    log_probs = []\n",
        "    values    = []\n",
        "    states    = []\n",
        "    actions   = []\n",
        "    rewards   = []\n",
        "    masks     = []\n",
        "    entropy = 0\n",
        "\n",
        "    for _ in range(num_steps):\n",
        "        state = torch.FloatTensor(state).to(device)\n",
        "        dist, value = model(state)\n",
        "\n",
        "        action = dist.sample()\n",
        "        next_state, reward, done, _ = envs.step(action.cpu().numpy())\n",
        "\n",
        "        log_prob = dist.log_prob(action)\n",
        "        entropy += dist.entropy().mean()\n",
        "        \n",
        "        log_probs.append(log_prob)\n",
        "        values.append(value)\n",
        "        rewards.append(torch.FloatTensor(reward).unsqueeze(1).to(device))\n",
        "        masks.append(torch.FloatTensor(1 - done).unsqueeze(1).to(device))\n",
        "        \n",
        "        states.append(state)\n",
        "        actions.append(action)\n",
        "        \n",
        "        state = next_state\n",
        "        frame_idx += 1\n",
        "        \n",
        "        if frame_idx % 1000 == 0:\n",
        "            test_reward = np.mean([test_env() for _ in range(10)])\n",
        "            test_rewards.append(test_reward)\n",
        "            plot(frame_idx, test_rewards)\n",
        "            if test_reward > threshold_reward: early_stop = True\n",
        "            \n",
        "\n",
        "    next_state = torch.FloatTensor(next_state).to(device)\n",
        "    _, next_value = model(next_state)\n",
        "    returns = compute_gae(next_value, rewards, masks, values)\n",
        "\n",
        "    returns   = torch.cat(returns).detach()\n",
        "    log_probs = torch.cat(log_probs).detach()\n",
        "    values    = torch.cat(values).detach()\n",
        "    states    = torch.cat(states)\n",
        "    actions   = torch.cat(actions)\n",
        "    advantage = returns - values\n",
        "    \n",
        "    ppo_update(ppo_epochs, mini_batch_size, states, actions, log_probs, returns, advantage)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1440x360 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J_7m5N3iLUW_"
      },
      "source": [
        "<h1>Saving trajectories for GAIL</h1>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M7IXR4DLLUXA",
        "outputId": "9aa2522e-6b67-4e31-c4cf-f34c8d5b85c2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from itertools import count\n",
        "\n",
        "max_expert_num = 50000\n",
        "num_steps = 0\n",
        "expert_traj = []\n",
        "\n",
        "for i_episode in count():\n",
        "    state = env.reset()\n",
        "    done = False\n",
        "    total_reward = 0\n",
        "    \n",
        "    while not done:\n",
        "        state = torch.FloatTensor(state).unsqueeze(0).to(device)\n",
        "        dist, _ = model(state)\n",
        "        action = dist.sample().cpu().numpy()[0]\n",
        "        next_state, reward, done, _ = env.step(action)\n",
        "        state = next_state\n",
        "        total_reward += reward\n",
        "        expert_traj.append(np.hstack([state, action]))\n",
        "        num_steps += 1\n",
        "    \n",
        "    print(\"episode:\", i_episode, \"reward:\", total_reward)\n",
        "    \n",
        "    if num_steps >= max_expert_num:\n",
        "        break\n",
        "        \n",
        "expert_traj = np.stack(expert_traj)\n",
        "print()\n",
        "print(expert_traj.shape)\n",
        "print()\n",
        "np.save(\"expert_traj.npy\", expert_traj)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "episode: 0 reward: -1112.960701221157\n",
            "episode: 1 reward: -1099.294586624267\n",
            "episode: 2 reward: -1072.3373977836793\n",
            "episode: 3 reward: -1050.8183438912797\n",
            "episode: 4 reward: -776.7683648068127\n",
            "episode: 5 reward: -862.731100080095\n",
            "episode: 6 reward: -944.7533694225656\n",
            "episode: 7 reward: -808.382836419688\n",
            "episode: 8 reward: -747.32844265883\n",
            "episode: 9 reward: -1057.0305006092685\n",
            "episode: 10 reward: -965.6968636810801\n",
            "episode: 11 reward: -1025.8448809037295\n",
            "episode: 12 reward: -892.4005513182923\n",
            "episode: 13 reward: -1209.8643579567552\n",
            "episode: 14 reward: -864.4912306741742\n",
            "episode: 15 reward: -1021.2137389358027\n",
            "episode: 16 reward: -963.2342300872597\n",
            "episode: 17 reward: -869.6196329870959\n",
            "episode: 18 reward: -972.2461765410835\n",
            "episode: 19 reward: -1016.9849153215393\n",
            "episode: 20 reward: -1196.314071950579\n",
            "episode: 21 reward: -1029.715352273832\n",
            "episode: 22 reward: -1063.0013763595127\n",
            "episode: 23 reward: -976.3640974600534\n",
            "episode: 24 reward: -1484.776722920955\n",
            "episode: 25 reward: -952.6895556748916\n",
            "episode: 26 reward: -1166.142729589362\n",
            "episode: 27 reward: -786.6850127402629\n",
            "episode: 28 reward: -1092.7410242011822\n",
            "episode: 29 reward: -956.4460207800925\n",
            "episode: 30 reward: -1060.7233136374734\n",
            "episode: 31 reward: -865.9011942155278\n",
            "episode: 32 reward: -1113.198839664843\n",
            "episode: 33 reward: -984.4070195697975\n",
            "episode: 34 reward: -875.088408543495\n",
            "episode: 35 reward: -868.6175853177965\n",
            "episode: 36 reward: -1074.5486997284042\n",
            "episode: 37 reward: -858.5360095365879\n",
            "episode: 38 reward: -969.3218971833567\n",
            "episode: 39 reward: -874.4983901934181\n",
            "episode: 40 reward: -929.8864078434534\n",
            "episode: 41 reward: -1157.1078338880125\n",
            "episode: 42 reward: -862.6263023508938\n",
            "episode: 43 reward: -1234.2790851016446\n",
            "episode: 44 reward: -1072.605379264547\n",
            "episode: 45 reward: -992.3234005169695\n",
            "episode: 46 reward: -1171.2226402812826\n",
            "episode: 47 reward: -743.0810796660993\n",
            "episode: 48 reward: -969.7334515667931\n",
            "episode: 49 reward: -1021.0636818259945\n",
            "episode: 50 reward: -1024.019645568928\n",
            "episode: 51 reward: -988.2230408283248\n",
            "episode: 52 reward: -874.6602682910292\n",
            "episode: 53 reward: -1140.4433172987458\n",
            "episode: 54 reward: -1025.064563906936\n",
            "episode: 55 reward: -869.3854832783611\n",
            "episode: 56 reward: -882.3299458178946\n",
            "episode: 57 reward: -1035.187272737835\n",
            "episode: 58 reward: -1075.925962939595\n",
            "episode: 59 reward: -1002.5974669995833\n",
            "episode: 60 reward: -1239.7635700167023\n",
            "episode: 61 reward: -768.7215787477003\n",
            "episode: 62 reward: -952.4052577046882\n",
            "episode: 63 reward: -994.9422302971906\n",
            "episode: 64 reward: -907.1988823458485\n",
            "episode: 65 reward: -1099.2642486663615\n",
            "episode: 66 reward: -993.9120707596975\n",
            "episode: 67 reward: -1282.8575080404778\n",
            "episode: 68 reward: -1092.1530359567075\n",
            "episode: 69 reward: -1163.320696353574\n",
            "episode: 70 reward: -1176.0215390963237\n",
            "episode: 71 reward: -892.4568760853264\n",
            "episode: 72 reward: -875.6094567496283\n",
            "episode: 73 reward: -1057.2273595787863\n",
            "episode: 74 reward: -893.8935273666932\n",
            "episode: 75 reward: -862.8701820261308\n",
            "episode: 76 reward: -1259.6030109000537\n",
            "episode: 77 reward: -1002.6021216115445\n",
            "episode: 78 reward: -962.4879780339518\n",
            "episode: 79 reward: -1152.4931669682867\n",
            "episode: 80 reward: -750.1356723761927\n",
            "episode: 81 reward: -1066.8358124204963\n",
            "episode: 82 reward: -888.2940991709886\n",
            "episode: 83 reward: -870.9819783027958\n",
            "episode: 84 reward: -864.1282179800352\n",
            "episode: 85 reward: -1063.6411774316155\n",
            "episode: 86 reward: -1010.1798702915897\n",
            "episode: 87 reward: -865.8135782777692\n",
            "episode: 88 reward: -1284.177680263042\n",
            "episode: 89 reward: -877.1655819619426\n",
            "episode: 90 reward: -871.6230381512228\n",
            "episode: 91 reward: -1062.1358342134301\n",
            "episode: 92 reward: -990.9749577777643\n",
            "episode: 93 reward: -975.5432387797878\n",
            "episode: 94 reward: -1007.7323962790014\n",
            "episode: 95 reward: -1084.076046366689\n",
            "episode: 96 reward: -988.1874985673202\n",
            "episode: 97 reward: -756.9932419746376\n",
            "episode: 98 reward: -885.2651617738368\n",
            "episode: 99 reward: -962.7813889761052\n",
            "episode: 100 reward: -1133.8949589605481\n",
            "episode: 101 reward: -859.9030246281075\n",
            "episode: 102 reward: -683.3052732937592\n",
            "episode: 103 reward: -978.0765452190423\n",
            "episode: 104 reward: -1301.0954516355473\n",
            "episode: 105 reward: -967.8117120795574\n",
            "episode: 106 reward: -1143.054760382335\n",
            "episode: 107 reward: -1141.554571028155\n",
            "episode: 108 reward: -1192.2382899787744\n",
            "episode: 109 reward: -1242.6411744672137\n",
            "episode: 110 reward: -830.397449991727\n",
            "episode: 111 reward: -1058.5308592605902\n",
            "episode: 112 reward: -1190.459225754924\n",
            "episode: 113 reward: -1071.9233764813948\n",
            "episode: 114 reward: -762.2347417193553\n",
            "episode: 115 reward: -1061.417121191303\n",
            "episode: 116 reward: -882.7158569031656\n",
            "episode: 117 reward: -1169.4017471012437\n",
            "episode: 118 reward: -985.9566359624249\n",
            "episode: 119 reward: -869.1284205726373\n",
            "episode: 120 reward: -740.9699278495137\n",
            "episode: 121 reward: -1143.4206996536866\n",
            "episode: 122 reward: -853.2182553643619\n",
            "episode: 123 reward: -909.0482964393402\n",
            "episode: 124 reward: -1198.2423114950836\n",
            "episode: 125 reward: -1088.445493783775\n",
            "episode: 126 reward: -961.5534498119666\n",
            "episode: 127 reward: -768.8063561943899\n",
            "episode: 128 reward: -1336.4324110370308\n",
            "episode: 129 reward: -1050.902377043294\n",
            "episode: 130 reward: -883.1329413960592\n",
            "episode: 131 reward: -1296.2253016754983\n",
            "episode: 132 reward: -951.5675924817614\n",
            "episode: 133 reward: -891.6428453501103\n",
            "episode: 134 reward: -1147.3495995042574\n",
            "episode: 135 reward: -1041.3936833455073\n",
            "episode: 136 reward: -971.9940600977034\n",
            "episode: 137 reward: -1011.9346145696236\n",
            "episode: 138 reward: -845.0052240369313\n",
            "episode: 139 reward: -1050.044964923321\n",
            "episode: 140 reward: -1154.9042034356892\n",
            "episode: 141 reward: -967.5080617549339\n",
            "episode: 142 reward: -1002.5350407113975\n",
            "episode: 143 reward: -889.7884932428674\n",
            "episode: 144 reward: -965.9303614337923\n",
            "episode: 145 reward: -995.3676545779396\n",
            "episode: 146 reward: -1080.429852048818\n",
            "episode: 147 reward: -1167.1587824024648\n",
            "episode: 148 reward: -1087.0684395027222\n",
            "episode: 149 reward: -1347.8921394975441\n",
            "episode: 150 reward: -903.7271855361457\n",
            "episode: 151 reward: -852.027785609695\n",
            "episode: 152 reward: -942.0203181897894\n",
            "episode: 153 reward: -873.0809195013885\n",
            "episode: 154 reward: -879.1938258596233\n",
            "episode: 155 reward: -986.031670376717\n",
            "episode: 156 reward: -880.1223933562337\n",
            "episode: 157 reward: -866.2872490593282\n",
            "episode: 158 reward: -969.4309429513679\n",
            "episode: 159 reward: -1068.7006556311464\n",
            "episode: 160 reward: -994.7123650850053\n",
            "episode: 161 reward: -864.4465868527385\n",
            "episode: 162 reward: -1179.7187150521431\n",
            "episode: 163 reward: -1028.7213307546806\n",
            "episode: 164 reward: -889.7254676145908\n",
            "episode: 165 reward: -970.5902159802266\n",
            "episode: 166 reward: -953.3643824994372\n",
            "episode: 167 reward: -1049.649689981723\n",
            "episode: 168 reward: -776.1453398368398\n",
            "episode: 169 reward: -869.5838537302716\n",
            "episode: 170 reward: -904.2604071551576\n",
            "episode: 171 reward: -1017.7795640537058\n",
            "episode: 172 reward: -1028.2451253793636\n",
            "episode: 173 reward: -991.4001342491399\n",
            "episode: 174 reward: -1161.2391645973075\n",
            "episode: 175 reward: -998.7546816657236\n",
            "episode: 176 reward: -874.0920276901337\n",
            "episode: 177 reward: -907.0700172117208\n",
            "episode: 178 reward: -857.8291167659783\n",
            "episode: 179 reward: -986.5875924629197\n",
            "episode: 180 reward: -748.7666836116998\n",
            "episode: 181 reward: -1169.6446579499334\n",
            "episode: 182 reward: -890.5923742260427\n",
            "episode: 183 reward: -1088.9792209933516\n",
            "episode: 184 reward: -1149.8652122111607\n",
            "episode: 185 reward: -908.3941654441601\n",
            "episode: 186 reward: -1154.425601676496\n",
            "episode: 187 reward: -1182.1972705929265\n",
            "episode: 188 reward: -853.0175816134587\n",
            "episode: 189 reward: -883.8712597595699\n",
            "episode: 190 reward: -1303.1330322744138\n",
            "episode: 191 reward: -1052.2433921975685\n",
            "episode: 192 reward: -837.4014526068804\n",
            "episode: 193 reward: -1010.4643898289233\n",
            "episode: 194 reward: -1303.6131793121153\n",
            "episode: 195 reward: -1162.0435686138935\n",
            "episode: 196 reward: -1044.4306181194431\n",
            "episode: 197 reward: -1175.985522852221\n",
            "episode: 198 reward: -985.8845817606489\n",
            "episode: 199 reward: -1087.4435994747573\n",
            "episode: 200 reward: -1015.205324581346\n",
            "episode: 201 reward: -1249.724637908618\n",
            "episode: 202 reward: -983.100164410119\n",
            "episode: 203 reward: -975.589689649126\n",
            "episode: 204 reward: -901.8937794133797\n",
            "episode: 205 reward: -1148.8382439500595\n",
            "episode: 206 reward: -1188.2936153314786\n",
            "episode: 207 reward: -856.6683383523165\n",
            "episode: 208 reward: -989.6447333139565\n",
            "episode: 209 reward: -1172.681075310866\n",
            "episode: 210 reward: -1141.245496342499\n",
            "episode: 211 reward: -1137.5277132309727\n",
            "episode: 212 reward: -988.7580335095519\n",
            "episode: 213 reward: -1059.4559168676549\n",
            "episode: 214 reward: -863.7701706537382\n",
            "episode: 215 reward: -874.8647152860806\n",
            "episode: 216 reward: -1056.7616692552895\n",
            "episode: 217 reward: -1122.611454891744\n",
            "episode: 218 reward: -786.2179188804059\n",
            "episode: 219 reward: -1270.533631893492\n",
            "episode: 220 reward: -983.7599329486516\n",
            "episode: 221 reward: -1172.728930933134\n",
            "episode: 222 reward: -844.1661047006761\n",
            "episode: 223 reward: -855.5645613947585\n",
            "episode: 224 reward: -1248.7046474093286\n",
            "episode: 225 reward: -866.2833929056719\n",
            "episode: 226 reward: -887.2080021956762\n",
            "episode: 227 reward: -844.5512064325644\n",
            "episode: 228 reward: -1078.4943329689247\n",
            "episode: 229 reward: -995.0075210030568\n",
            "episode: 230 reward: -942.0860459832371\n",
            "episode: 231 reward: -1286.9524127920038\n",
            "episode: 232 reward: -972.021766251373\n",
            "episode: 233 reward: -881.9073017384072\n",
            "episode: 234 reward: -1063.6062077679128\n",
            "episode: 235 reward: -947.0125307876119\n",
            "episode: 236 reward: -872.0328003931703\n",
            "episode: 237 reward: -982.2262258301141\n",
            "episode: 238 reward: -980.9380546686378\n",
            "episode: 239 reward: -983.1869382743004\n",
            "episode: 240 reward: -971.3745420679231\n",
            "episode: 241 reward: -922.9477218357774\n",
            "episode: 242 reward: -1144.4507773132432\n",
            "episode: 243 reward: -867.2861179125962\n",
            "episode: 244 reward: -980.5450675427016\n",
            "episode: 245 reward: -958.6798228078682\n",
            "episode: 246 reward: -901.3963713927465\n",
            "episode: 247 reward: -940.9960723263631\n",
            "episode: 248 reward: -1079.3079056231456\n",
            "episode: 249 reward: -1286.8977545837004\n",
            "\n",
            "(50000, 4)\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}