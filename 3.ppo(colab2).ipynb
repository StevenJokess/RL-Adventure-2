{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python [conda env:pytorch4]",
      "language": "python",
      "name": "conda-env-pytorch4-py"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.5.5"
    },
    "colab": {
      "name": "3.ppo.ipynb",
      "provenance": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "PbaHh0K1LqEY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "324183a4-6833-47a6-842d-7c4d961d0246"
      },
      "source": [
        "!pip install git+https://github.com/StevenJokess/RL-Adventure-2\r\n",
        "!git clone https://github.com/StevenJokess/RL-Adventure-2\r\n",
        "%cd RL-Adventure-2\r\n",
        "!ls"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting git+https://github.com/StevenJokess/RL-Adventure-2\n",
            "  Cloning https://github.com/StevenJokess/RL-Adventure-2 to /tmp/pip-req-build-_1t5rv_9\n",
            "  Running command git clone -q https://github.com/StevenJokess/RL-Adventure-2 /tmp/pip-req-build-_1t5rv_9\n",
            "\u001b[31mERROR: Command errored out with exit status 1: python setup.py egg_info Check the logs for full command output.\u001b[0m\n",
            "Cloning into 'RL-Adventure-2'...\n",
            "remote: Enumerating objects: 28, done.\u001b[K\n",
            "remote: Counting objects: 100% (28/28), done.\u001b[K\n",
            "remote: Compressing objects: 100% (27/27), done.\u001b[K\n",
            "remote: Total 66 (delta 10), reused 0 (delta 0), pack-reused 38\u001b[K\n",
            "Unpacking objects: 100% (66/66), done.\n",
            "/content/RL-Adventure-2\n",
            "'1.actor-critic(colab).ipynb'   5.ddpg.ipynb\n",
            " 1.actor-critic.ipynb\t       '6.td3(colab).ipynb'\n",
            "'2.gae(colab).ipynb'\t        6.td3.ipynb\n",
            " 2.gae.ipynb\t\t       '7.soft actor-critic(colab).ipynb'\n",
            "'3.ppo(colab).ipynb'\t       '7.soft actor-critic.ipynb'\n",
            " 3.ppo.ipynb\t\t        8.gail.ipynb\n",
            "'4.acer(colab).ipynb'\t        9.her.ipynb\n",
            " 4.acer.ipynb\t\t        common\n",
            "'5.ddpg(colab).ipynb'\t        README.md\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q07DBS6-LUWr"
      },
      "source": [
        "import math\n",
        "import random\n",
        "\n",
        "import gym\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.distributions import Normal"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_l_OoOP7LUW0"
      },
      "source": [
        "from IPython.display import clear_output\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4P1Pfq0ELUW1"
      },
      "source": [
        "<h2>Use CUDA</h2>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Mr4HFkRLUW2"
      },
      "source": [
        "use_cuda = torch.cuda.is_available()\n",
        "device   = torch.device(\"cuda\" if use_cuda else \"cpu\")"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Zryqg1ULUW2"
      },
      "source": [
        "<h2>Create Environments</h2>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WGNIWlbiLUW2"
      },
      "source": [
        "from common.multiprocessing_env import SubprocVecEnv\n",
        "\n",
        "num_envs = 16\n",
        "env_name = \"Pendulum-v0\"\n",
        "\n",
        "def make_env():\n",
        "    def _thunk():\n",
        "        env = gym.make(env_name)\n",
        "        return env\n",
        "\n",
        "    return _thunk\n",
        "\n",
        "envs = [make_env() for i in range(num_envs)]\n",
        "envs = SubprocVecEnv(envs)\n",
        "\n",
        "env = gym.make(env_name)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Vt4ODueLUW3"
      },
      "source": [
        "<h2>Neural Network</h2>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G1j_Z3O3LUW3"
      },
      "source": [
        "def init_weights(m):\n",
        "    if isinstance(m, nn.Linear):\n",
        "        nn.init.normal_(m.weight, mean=0., std=0.1)\n",
        "        nn.init.constant_(m.bias, 0.1)\n",
        "        \n",
        "\n",
        "class ActorCritic(nn.Module):\n",
        "    def __init__(self, num_inputs, num_outputs, hidden_size, std=0.0):\n",
        "        super(ActorCritic, self).__init__()\n",
        "        \n",
        "        self.critic = nn.Sequential(\n",
        "            nn.Linear(num_inputs, hidden_size),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_size, 1)\n",
        "        )\n",
        "        \n",
        "        self.actor = nn.Sequential(\n",
        "            nn.Linear(num_inputs, hidden_size),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_size, num_outputs),\n",
        "        )\n",
        "        self.log_std = nn.Parameter(torch.ones(1, num_outputs) * std)\n",
        "        \n",
        "        self.apply(init_weights)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        value = self.critic(x)\n",
        "        mu    = self.actor(x)\n",
        "        std   = self.log_std.exp().expand_as(mu)\n",
        "        dist  = Normal(mu, std)\n",
        "        return dist, value"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mZXs_HY6LUW3"
      },
      "source": [
        "def plot(frame_idx, rewards):\n",
        "    clear_output(True)\n",
        "    plt.figure(figsize=(20,5))\n",
        "    plt.subplot(131)\n",
        "    plt.title('frame %s. reward: %s' % (frame_idx, rewards[-1]))\n",
        "    plt.plot(rewards)\n",
        "    plt.show()\n",
        "    \n",
        "def test_env(vis=False):\n",
        "    state = env.reset()\n",
        "    if vis: env.render()\n",
        "    done = False\n",
        "    total_reward = 0\n",
        "    while not done:\n",
        "        state = torch.FloatTensor(state).unsqueeze(0).to(device)\n",
        "        dist, _ = model(state)\n",
        "        next_state, reward, done, _ = env.step(dist.sample().cpu().numpy()[0])\n",
        "        state = next_state\n",
        "        if vis: env.render()\n",
        "        total_reward += reward\n",
        "    return total_reward"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zEwDh2ZGLUW4"
      },
      "source": [
        "<h2>GAE</h2>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yDEGeHD6LUW4"
      },
      "source": [
        "def compute_gae(next_value, rewards, masks, values, gamma=0.99, tau=0.95):\n",
        "    values = values + [next_value]\n",
        "    gae = 0\n",
        "    returns = []\n",
        "    for step in reversed(range(len(rewards))):\n",
        "        delta = rewards[step] + gamma * values[step + 1] * masks[step] - values[step]\n",
        "        gae = delta + gamma * tau * masks[step] * gae\n",
        "        returns.insert(0, gae + values[step])\n",
        "    return returns"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZFOVwofxLUW5"
      },
      "source": [
        "<h1> Proximal Policy Optimization Algorithm</h1>\n",
        "<h2><a href=\"https://arxiv.org/abs/1707.06347\">Arxiv</a></h2>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rO9VDU-kLUW6"
      },
      "source": [
        "def ppo_iter(mini_batch_size, states, actions, log_probs, returns, advantage):\n",
        "    batch_size = states.size(0)\n",
        "    for _ in range(batch_size // mini_batch_size):\n",
        "        rand_ids = np.random.randint(0, batch_size, mini_batch_size)\n",
        "        yield states[rand_ids, :], actions[rand_ids, :], log_probs[rand_ids, :], returns[rand_ids, :], advantage[rand_ids, :]\n",
        "        \n",
        "        \n",
        "\n",
        "def ppo_update(ppo_epochs, mini_batch_size, states, actions, log_probs, returns, advantages, clip_param=0.2):\n",
        "    for _ in range(ppo_epochs):\n",
        "        for state, action, old_log_probs, return_, advantage in ppo_iter(mini_batch_size, states, actions, log_probs, returns, advantages):\n",
        "            dist, value = model(state)\n",
        "            entropy = dist.entropy().mean()\n",
        "            new_log_probs = dist.log_prob(action)\n",
        "\n",
        "            ratio = (new_log_probs - old_log_probs).exp()\n",
        "            surr1 = ratio * advantage\n",
        "            surr2 = torch.clamp(ratio, 1.0 - clip_param, 1.0 + clip_param) * advantage\n",
        "\n",
        "            actor_loss  = - torch.min(surr1, surr2).mean()\n",
        "            critic_loss = (return_ - value).pow(2).mean()\n",
        "\n",
        "            loss = 0.5 * critic_loss + actor_loss - 0.001 * entropy\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9XNfADArLUW7"
      },
      "source": [
        "num_inputs  = envs.observation_space.shape[0]\n",
        "num_outputs = envs.action_space.shape[0]\n",
        "\n",
        "#Hyper params:\n",
        "hidden_size      = 256\n",
        "lr               = 3e-4\n",
        "num_steps        = 20\n",
        "mini_batch_size  = 5\n",
        "ppo_epochs       = 4\n",
        "threshold_reward = -200\n",
        "\n",
        "model = ActorCritic(num_inputs, num_outputs, hidden_size).to(device)\n",
        "optimizer = optim.Adam(model.parameters(), lr=lr)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wE7Ly8vhLUW7"
      },
      "source": [
        "max_frames = 4000 #15000\n",
        "frame_idx  = 0\n",
        "test_rewards = []"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FN55ZWVJLUW8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 336
        },
        "outputId": "dddf7310-fbfe-4729-8019-4b140e4a742e"
      },
      "source": [
        "state = envs.reset()\n",
        "early_stop = False\n",
        "\n",
        "while frame_idx < max_frames and not early_stop:\n",
        "\n",
        "    log_probs = []\n",
        "    values    = []\n",
        "    states    = []\n",
        "    actions   = []\n",
        "    rewards   = []\n",
        "    masks     = []\n",
        "    entropy = 0\n",
        "\n",
        "    for _ in range(num_steps):\n",
        "        state = torch.FloatTensor(state).to(device)\n",
        "        dist, value = model(state)\n",
        "\n",
        "        action = dist.sample()\n",
        "        next_state, reward, done, _ = envs.step(action.cpu().numpy())\n",
        "\n",
        "        log_prob = dist.log_prob(action)\n",
        "        entropy += dist.entropy().mean()\n",
        "        \n",
        "        log_probs.append(log_prob)\n",
        "        values.append(value)\n",
        "        rewards.append(torch.FloatTensor(reward).unsqueeze(1).to(device))\n",
        "        masks.append(torch.FloatTensor(1 - done).unsqueeze(1).to(device))\n",
        "        \n",
        "        states.append(state)\n",
        "        actions.append(action)\n",
        "        \n",
        "        state = next_state\n",
        "        frame_idx += 1\n",
        "        \n",
        "        if frame_idx % 1000 == 0:\n",
        "            test_reward = np.mean([test_env() for _ in range(10)])\n",
        "            test_rewards.append(test_reward)\n",
        "            plot(frame_idx, test_rewards)\n",
        "            if test_reward > threshold_reward: early_stop = True\n",
        "            \n",
        "\n",
        "    next_state = torch.FloatTensor(next_state).to(device)\n",
        "    _, next_value = model(next_state)\n",
        "    returns = compute_gae(next_value, rewards, masks, values)\n",
        "\n",
        "    returns   = torch.cat(returns).detach()\n",
        "    log_probs = torch.cat(log_probs).detach()\n",
        "    values    = torch.cat(values).detach()\n",
        "    states    = torch.cat(states)\n",
        "    actions   = torch.cat(actions)\n",
        "    advantage = returns - values\n",
        "    \n",
        "    ppo_update(ppo_epochs, mini_batch_size, states, actions, log_probs, returns, advantage)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1440x360 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J_7m5N3iLUW_"
      },
      "source": [
        "<h1>Saving trajectories for GAIL</h1>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M7IXR4DLLUXA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e4ef8ecd-c715-4390-a3cc-56072a762a07"
      },
      "source": [
        "from itertools import count\n",
        "\n",
        "max_expert_num = 50000\n",
        "num_steps = 0\n",
        "expert_traj = []\n",
        "\n",
        "for i_episode in count():\n",
        "    state = env.reset()\n",
        "    done = False\n",
        "    total_reward = 0\n",
        "    \n",
        "    while not done:\n",
        "        state = torch.FloatTensor(state).unsqueeze(0).to(device)\n",
        "        dist, _ = model(state)\n",
        "        action = dist.sample().cpu().numpy()[0]\n",
        "        next_state, reward, done, _ = env.step(action)\n",
        "        state = next_state\n",
        "        total_reward += reward\n",
        "        expert_traj.append(np.hstack([state, action]))\n",
        "        num_steps += 1\n",
        "    \n",
        "    print(\"episode:\", i_episode, \"reward:\", total_reward)\n",
        "    \n",
        "    if num_steps >= max_expert_num:\n",
        "        break\n",
        "        \n",
        "expert_traj = np.stack(expert_traj)\n",
        "print()\n",
        "print(expert_traj.shape)\n",
        "print()\n",
        "np.save(\"expert_traj.npy\", expert_traj)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "episode: 0 reward: -1365.0182666898022\n",
            "episode: 1 reward: -1398.47763874739\n",
            "episode: 2 reward: -1482.6954077995838\n",
            "episode: 3 reward: -802.1188629895494\n",
            "episode: 4 reward: -1399.1008982871253\n",
            "episode: 5 reward: -1578.7116847237805\n",
            "episode: 6 reward: -1503.1010763878448\n",
            "episode: 7 reward: -1339.4541090901294\n",
            "episode: 8 reward: -1470.1698524482777\n",
            "episode: 9 reward: -1623.425814200073\n",
            "episode: 10 reward: -1330.6833987936386\n",
            "episode: 11 reward: -1503.7448909429938\n",
            "episode: 12 reward: -1482.6363268797213\n",
            "episode: 13 reward: -1533.996626955547\n",
            "episode: 14 reward: -1255.0575394803254\n",
            "episode: 15 reward: -1612.5872759069766\n",
            "episode: 16 reward: -1345.0870051395493\n",
            "episode: 17 reward: -1397.566425022208\n",
            "episode: 18 reward: -1614.2813368996678\n",
            "episode: 19 reward: -1359.2140631639904\n",
            "episode: 20 reward: -1585.4407513622637\n",
            "episode: 21 reward: -1377.878556075817\n",
            "episode: 22 reward: -1354.6171232518318\n",
            "episode: 23 reward: -1471.5221233641273\n",
            "episode: 24 reward: -1345.6612448812962\n",
            "episode: 25 reward: -1325.4718574776025\n",
            "episode: 26 reward: -1391.5166669015439\n",
            "episode: 27 reward: -1587.4030672997076\n",
            "episode: 28 reward: -1366.3822881698907\n",
            "episode: 29 reward: -1242.4308024434067\n",
            "episode: 30 reward: -1395.5854943998745\n",
            "episode: 31 reward: -1356.2361494438119\n",
            "episode: 32 reward: -1429.4329211960576\n",
            "episode: 33 reward: -1508.1357950537654\n",
            "episode: 34 reward: -1502.9751302222896\n",
            "episode: 35 reward: -1096.8846003680433\n",
            "episode: 36 reward: -1487.835218684018\n",
            "episode: 37 reward: -1479.3233258019407\n",
            "episode: 38 reward: -871.1206031548862\n",
            "episode: 39 reward: -1419.3377316265194\n",
            "episode: 40 reward: -1398.1665321949029\n",
            "episode: 41 reward: -1385.028764479192\n",
            "episode: 42 reward: -1578.778257646773\n",
            "episode: 43 reward: -1570.8703340685129\n",
            "episode: 44 reward: -1465.0360134992138\n",
            "episode: 45 reward: -1590.3177099316752\n",
            "episode: 46 reward: -1420.2246311494698\n",
            "episode: 47 reward: -1369.9922009400195\n",
            "episode: 48 reward: -1423.0356725262093\n",
            "episode: 49 reward: -1231.5267385072134\n",
            "episode: 50 reward: -1601.2330630596177\n",
            "episode: 51 reward: -1326.1831987930855\n",
            "episode: 52 reward: -1456.050130785923\n",
            "episode: 53 reward: -1182.9224311312837\n",
            "episode: 54 reward: -796.3499700880661\n",
            "episode: 55 reward: -933.4336129160805\n",
            "episode: 56 reward: -1503.9176631317437\n",
            "episode: 57 reward: -1415.7665863597435\n",
            "episode: 58 reward: -1414.8095195354122\n",
            "episode: 59 reward: -1348.3395767938553\n",
            "episode: 60 reward: -1484.6467893616727\n",
            "episode: 61 reward: -1317.78748945872\n",
            "episode: 62 reward: -1526.1770964001018\n",
            "episode: 63 reward: -1405.6952745342394\n",
            "episode: 64 reward: -1304.6885315460424\n",
            "episode: 65 reward: -1461.8329067787859\n",
            "episode: 66 reward: -1369.244110800808\n",
            "episode: 67 reward: -1573.3071372272732\n",
            "episode: 68 reward: -1554.8632438836685\n",
            "episode: 69 reward: -1586.1611193125298\n",
            "episode: 70 reward: -1466.2959540982156\n",
            "episode: 71 reward: -1434.3782890632674\n",
            "episode: 72 reward: -1392.4186684578235\n",
            "episode: 73 reward: -1601.20259752735\n",
            "episode: 74 reward: -1259.4303121153146\n",
            "episode: 75 reward: -1612.6388807725336\n",
            "episode: 76 reward: -1455.8448964417978\n",
            "episode: 77 reward: -1371.5129675801536\n",
            "episode: 78 reward: -1220.0294411983991\n",
            "episode: 79 reward: -1384.4879357879267\n",
            "episode: 80 reward: -1465.081127007882\n",
            "episode: 81 reward: -1529.7604324363163\n",
            "episode: 82 reward: -1504.0487151107975\n",
            "episode: 83 reward: -1446.779225518959\n",
            "episode: 84 reward: -1159.7935959140846\n",
            "episode: 85 reward: -1293.471915709215\n",
            "episode: 86 reward: -1583.3650337919084\n",
            "episode: 87 reward: -1500.7067764799547\n",
            "episode: 88 reward: -1424.355678519798\n",
            "episode: 89 reward: -1281.200703877871\n",
            "episode: 90 reward: -1439.7716078825604\n",
            "episode: 91 reward: -1489.3391381759182\n",
            "episode: 92 reward: -1568.1418804852508\n",
            "episode: 93 reward: -646.9405941917025\n",
            "episode: 94 reward: -1453.1568020404488\n",
            "episode: 95 reward: -1378.9445902539828\n",
            "episode: 96 reward: -1341.6201109637666\n",
            "episode: 97 reward: -1352.0231482272807\n",
            "episode: 98 reward: -1372.2679105623863\n",
            "episode: 99 reward: -1367.5029982221085\n",
            "episode: 100 reward: -1411.2628557368603\n",
            "episode: 101 reward: -1562.1572296401473\n",
            "episode: 102 reward: -1368.980303414058\n",
            "episode: 103 reward: -1519.7823960053072\n",
            "episode: 104 reward: -1501.294010811524\n",
            "episode: 105 reward: -1275.5323501480498\n",
            "episode: 106 reward: -1520.804716515801\n",
            "episode: 107 reward: -1277.859952245092\n",
            "episode: 108 reward: -1391.0107551817653\n",
            "episode: 109 reward: -1494.885512432868\n",
            "episode: 110 reward: -1632.4162011131534\n",
            "episode: 111 reward: -1503.7735343062984\n",
            "episode: 112 reward: -1511.0745654375633\n",
            "episode: 113 reward: -1251.15236169066\n",
            "episode: 114 reward: -1630.8759220593765\n",
            "episode: 115 reward: -1540.845013600204\n",
            "episode: 116 reward: -1505.7209998467204\n",
            "episode: 117 reward: -970.0754592115511\n",
            "episode: 118 reward: -1230.0932087415679\n",
            "episode: 119 reward: -1536.5436438729841\n",
            "episode: 120 reward: -1588.6976718920237\n",
            "episode: 121 reward: -1233.8993024919343\n",
            "episode: 122 reward: -1586.6771189833726\n",
            "episode: 123 reward: -1180.239592055464\n",
            "episode: 124 reward: -1394.65311845967\n",
            "episode: 125 reward: -1625.990012306521\n",
            "episode: 126 reward: -1128.7181203735622\n",
            "episode: 127 reward: -1404.3687982963795\n",
            "episode: 128 reward: -1437.133347700616\n",
            "episode: 129 reward: -1566.9720891258225\n",
            "episode: 130 reward: -1235.3621234136085\n",
            "episode: 131 reward: -1364.6984584741224\n",
            "episode: 132 reward: -1104.0419997640586\n",
            "episode: 133 reward: -1281.0836110936445\n",
            "episode: 134 reward: -1503.3681306274311\n",
            "episode: 135 reward: -1555.5905292368132\n",
            "episode: 136 reward: -1093.156781269318\n",
            "episode: 137 reward: -1553.885732837627\n",
            "episode: 138 reward: -1369.5229607644824\n",
            "episode: 139 reward: -1425.3478489367335\n",
            "episode: 140 reward: -1346.8369513912307\n",
            "episode: 141 reward: -1418.7387313257125\n",
            "episode: 142 reward: -985.9297425436329\n",
            "episode: 143 reward: -1369.156022195542\n",
            "episode: 144 reward: -1494.7153455298358\n",
            "episode: 145 reward: -1432.9154121997185\n",
            "episode: 146 reward: -1076.1943895346633\n",
            "episode: 147 reward: -1198.8695743537464\n",
            "episode: 148 reward: -1476.2491613880823\n",
            "episode: 149 reward: -1486.56238959724\n",
            "episode: 150 reward: -974.8953804469998\n",
            "episode: 151 reward: -1320.5651283908956\n",
            "episode: 152 reward: -1499.8358391765034\n",
            "episode: 153 reward: -1616.5255485959021\n",
            "episode: 154 reward: -1095.2583418496445\n",
            "episode: 155 reward: -1062.8160274729228\n",
            "episode: 156 reward: -1564.7601097623376\n",
            "episode: 157 reward: -1335.1367677637886\n",
            "episode: 158 reward: -1501.423485962709\n",
            "episode: 159 reward: -1393.4714433130152\n",
            "episode: 160 reward: -1539.5140421188419\n",
            "episode: 161 reward: -1411.3683995605636\n",
            "episode: 162 reward: -1469.221827910482\n",
            "episode: 163 reward: -1447.804753368119\n",
            "episode: 164 reward: -1547.6151058189305\n",
            "episode: 165 reward: -1251.383442983985\n",
            "episode: 166 reward: -1413.412425687732\n",
            "episode: 167 reward: -1252.283254849853\n",
            "episode: 168 reward: -1362.8091338321717\n",
            "episode: 169 reward: -1425.0970344066666\n",
            "episode: 170 reward: -1607.7687596898322\n",
            "episode: 171 reward: -1493.350574184641\n",
            "episode: 172 reward: -1335.0060764796992\n",
            "episode: 173 reward: -1409.4104203519262\n",
            "episode: 174 reward: -1366.2787573577687\n",
            "episode: 175 reward: -722.1172885145188\n",
            "episode: 176 reward: -1353.1530887489948\n",
            "episode: 177 reward: -1400.8437620219893\n",
            "episode: 178 reward: -1343.8974547961627\n",
            "episode: 179 reward: -1374.5199700063129\n",
            "episode: 180 reward: -1553.7719395242182\n",
            "episode: 181 reward: -1299.0100591991027\n",
            "episode: 182 reward: -1506.0975096933848\n",
            "episode: 183 reward: -1621.7040847579033\n",
            "episode: 184 reward: -1447.6584649800475\n",
            "episode: 185 reward: -1402.3233030072065\n",
            "episode: 186 reward: -1577.047001041444\n",
            "episode: 187 reward: -1516.5575292374726\n",
            "episode: 188 reward: -1596.3691255913193\n",
            "episode: 189 reward: -1502.2174573303382\n",
            "episode: 190 reward: -1395.3469417876395\n",
            "episode: 191 reward: -1177.804649158186\n",
            "episode: 192 reward: -1484.2946130115563\n",
            "episode: 193 reward: -1379.6967192878271\n",
            "episode: 194 reward: -1367.936620896538\n",
            "episode: 195 reward: -1378.083774004822\n",
            "episode: 196 reward: -1315.2916347469898\n",
            "episode: 197 reward: -1382.5576137169583\n",
            "episode: 198 reward: -1396.9215442517088\n",
            "episode: 199 reward: -1367.1662498783933\n",
            "episode: 200 reward: -1352.9709002119296\n",
            "episode: 201 reward: -1367.6265897343983\n",
            "episode: 202 reward: -816.3021119778576\n",
            "episode: 203 reward: -1554.515592058231\n",
            "episode: 204 reward: -1570.543075900169\n",
            "episode: 205 reward: -1293.1182188815826\n",
            "episode: 206 reward: -1469.8880243353108\n",
            "episode: 207 reward: -1382.6735288932061\n",
            "episode: 208 reward: -1454.6468011896916\n",
            "episode: 209 reward: -1288.2548795650764\n",
            "episode: 210 reward: -1392.2433053661978\n",
            "episode: 211 reward: -1410.1908988691355\n",
            "episode: 212 reward: -1481.3952193127373\n",
            "episode: 213 reward: -1245.0832125785312\n",
            "episode: 214 reward: -1359.7410172543098\n",
            "episode: 215 reward: -1378.479237625807\n",
            "episode: 216 reward: -1228.1449037205984\n",
            "episode: 217 reward: -1435.090213699615\n",
            "episode: 218 reward: -1420.3265697949512\n",
            "episode: 219 reward: -1567.574963878455\n",
            "episode: 220 reward: -1375.1568117721044\n",
            "episode: 221 reward: -1484.2216119248815\n",
            "episode: 222 reward: -1459.5452668404228\n",
            "episode: 223 reward: -1402.7796329857017\n",
            "episode: 224 reward: -1241.1716978147263\n",
            "episode: 225 reward: -1416.3873581196046\n",
            "episode: 226 reward: -1609.693891655136\n",
            "episode: 227 reward: -1498.1333525731393\n",
            "episode: 228 reward: -1504.0310804178234\n",
            "episode: 229 reward: -1452.5654986954166\n",
            "episode: 230 reward: -1560.9305562422794\n",
            "episode: 231 reward: -1492.8715979488613\n",
            "episode: 232 reward: -1546.6821207578575\n",
            "episode: 233 reward: -1406.9870028678763\n",
            "episode: 234 reward: -1594.2372001855401\n",
            "episode: 235 reward: -1534.2740285722227\n",
            "episode: 236 reward: -1488.5090032368976\n",
            "episode: 237 reward: -1242.0645950168725\n",
            "episode: 238 reward: -1552.1019791130811\n",
            "episode: 239 reward: -1392.222037687037\n",
            "episode: 240 reward: -1568.0658534675147\n",
            "episode: 241 reward: -1363.5750685659307\n",
            "episode: 242 reward: -1406.299675223161\n",
            "episode: 243 reward: -1599.2736595695092\n",
            "episode: 244 reward: -1558.0655501681752\n",
            "episode: 245 reward: -1361.8760307005873\n",
            "episode: 246 reward: -1504.7681033830559\n",
            "episode: 247 reward: -1469.9761927917582\n",
            "episode: 248 reward: -1528.9615316471577\n",
            "episode: 249 reward: -1338.3693315630328\n",
            "\n",
            "(50000, 4)\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eVrgEAyHxzVW"
      },
      "source": [
        "np.save(\"expert_traj.npy\", expert_traj)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 131
        },
        "id": "-s12fgOHx1MC",
        "outputId": "71f79f4e-f86d-421f-fc61-a32fc38f730c"
      },
      "source": [
        "np.save(\"expert_traj.npy\", expert_traj);!ls"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-15-b2ee519a5cb6>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    np.save(\"expert_traj.npy\", expert_traj);!ls\u001b[0m\n\u001b[0m                                            ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    }
  ]
}